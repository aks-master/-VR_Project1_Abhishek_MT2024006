{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import os  # For interacting with the operating system\n",
    "import cv2  # For image processing\n",
    "from sklearn.model_selection import train_test_split  # For splitting the dataset into training and testing sets\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  # For evaluating the model\n",
    "from sklearn.svm import SVC  # For Support Vector Machine classifier\n",
    "from sklearn.neural_network import MLPClassifier  # For Neural Network classifier\n",
    "from skimage.feature import hog  # For extracting Histogram of Oriented Gradients (HOG) features from images\n",
    "import tensorflow as tf  # For deep learning operations\n",
    "from tensorflow.keras.models import Sequential  # For creating a linear stack of layers for the neural network\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization  # For defining different types of layers in the neural network\n",
    "from tensorflow.keras.optimizers import Adam, SGD  # For optimizing the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to extract features from images\n",
    "\n",
    "def extract_features(image_path, use_hog):\n",
    "    \"\"\"Load image and extract features (HOG or raw pixels).\"\"\"\n",
    "    # Load the image in grayscale mode\n",
    "    image = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image not found or unable to load: {image_path}\")\n",
    "\n",
    "    # Resize the image to a fixed size (128x128)\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "\n",
    "    if use_hog:\n",
    "        # Extract HOG features for ML classifiers\n",
    "        features, _ = hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "        return features  # Return 1D feature vector\n",
    "    else:\n",
    "        # Normalize pixel values for CNN (range 0-1)\n",
    "        return image.reshape(128, 128, 1) / 255.0  # Return 2D image for CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A:\n",
    "Binary Classification Using Handcrafted Features and ML Classifiers (4 Marks) \n",
    "- i. Extract handcrafted features from the dataset. \n",
    "- ii. Train and evaluate at least two machine learning classifiers (e.g., SVM, Neural \n",
    "network) to classify faces as \"with mask\" or \"without mask.\" \n",
    "- iii. Report and compare the accuracy of the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (4095, 8100), Labels shape: (4095,)\n",
      "Number of 'with_mask' images: 2165\n",
      "Number of 'without_mask' images: 1930\n"
     ]
    }
   ],
   "source": [
    "# load the dataset for normal ML models require 1D feature vector (HOG)\n",
    "\n",
    "# Set dataset path\n",
    "dataset_path = \"dataset\"\n",
    "\n",
    "# Storage for dataset\n",
    "data = []\n",
    "labels = []\n",
    "label_counts = {'with_mask': 0, 'without_mask': 0}\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "for label in ['with_mask', 'without_mask']:\n",
    "    folder_path = os.path.join(dataset_path, label)\n",
    "    # print(f\"Processing folder: {folder_path}\")\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # print(f\"Processing file: {file_path}\")\n",
    "        try:\n",
    "            features = extract_features(file_path, use_hog=True)\n",
    "            data.append(features)\n",
    "            labels.append(1 if label == 'with_mask' else 0)\n",
    "            label_counts[label] += 1\n",
    "        except ValueError as e:\n",
    "            print(e) # Print the error message\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Print dataset shape\n",
    "print(f\"Feature array shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "print(f\"Number of 'with_mask' images: {label_counts['with_mask']}\")\n",
    "print(f\"Number of 'without_mask' images: {label_counts['without_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9304029304029304\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       366\n",
      "           1       0.91      0.96      0.94       453\n",
      "\n",
      "    accuracy                           0.93       819\n",
      "   macro avg       0.93      0.93      0.93       819\n",
      "weighted avg       0.93      0.93      0.93       819\n",
      "\n",
      "Confusion Matrix:\n",
      "[[325  41]\n",
      " [ 16 437]]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate SVM classifier\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Report the accuracy of the SVM classifier\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy: 0.894993894993895\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       366\n",
      "           1       0.90      0.91      0.91       453\n",
      "\n",
      "    accuracy                           0.89       819\n",
      "   macro avg       0.89      0.89      0.89       819\n",
      "weighted avg       0.89      0.89      0.89       819\n",
      "\n",
      "Confusion Matrix:\n",
      "[[319  47]\n",
      " [ 39 414]]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Neural Network classifier\n",
    "nn_clf = MLPClassifier()\n",
    "nn_clf.fit(X_train, y_train)\n",
    "y_pred_nn = nn_clf.predict(X_test)\n",
    "nn_accuracy = accuracy_score(y_test, y_pred_nn)\n",
    "\n",
    "# Report the accuracy of the MLP classifier\n",
    "print(f\"Neural Network Accuracy: {nn_accuracy}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (4095, 128, 128, 1), Labels shape: (4095,)\n",
      "Number of 'with_mask' images: 2165\n",
      "Number of 'without_mask' images: 1930\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset for CNN model require 2D image array\n",
    "# Set dataset path\n",
    "dataset_path = \"dataset\"\n",
    "\n",
    "# Storage for dataset\n",
    "data = []\n",
    "labels = []\n",
    "label_counts = {'with_mask': 0, 'without_mask': 0}\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "for label in ['with_mask', 'without_mask']:\n",
    "    folder_path = os.path.join(dataset_path, label)\n",
    "    # print(f\"Processing folder: {folder_path}\")\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # print(f\"Processing file: {file_path}\")\n",
    "        try:\n",
    "            features = extract_features(file_path, use_hog=False)\n",
    "            data.append(features)\n",
    "            labels.append(1 if label == 'with_mask' else 0)\n",
    "            label_counts[label] += 1\n",
    "        except ValueError as e:\n",
    "            print(e) # Print the error message\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Print dataset shape\n",
    "print(f\"Feature array shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "print(f\"Number of 'with_mask' images: {label_counts['with_mask']}\")\n",
    "print(f\"Number of 'without_mask' images: {label_counts['without_mask']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (3276, 128, 128, 1), (3276, 2)\n",
      "Testing set shape: (819, 128, 128, 1), (819, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode labels for categorical cross-entropy loss\n",
    "y = tf.keras.utils.to_categorical(y, 2)\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training CNN Model 1 (Adam, deeper layers, higher dropout)...\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 86s 830ms/step - loss: 0.5501 - accuracy: 0.7085 - val_loss: 0.3828 - val_accuracy: 0.8278\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 72s 700ms/step - loss: 0.3390 - accuracy: 0.8538 - val_loss: 0.2726 - val_accuracy: 0.8864\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 71s 686ms/step - loss: 0.2387 - accuracy: 0.9011 - val_loss: 0.2213 - val_accuracy: 0.9170\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 75s 730ms/step - loss: 0.1837 - accuracy: 0.9280 - val_loss: 0.1996 - val_accuracy: 0.9145\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 69s 667ms/step - loss: 0.1377 - accuracy: 0.9466 - val_loss: 0.1943 - val_accuracy: 0.9328\n",
      "26/26 [==============================] - 5s 207ms/step - loss: 0.1943 - accuracy: 0.9328\n",
      " CNN Model 1 Test Accuracy: 0.9328\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- CNN Model 1 --------------------------\n",
    "def create_cnn_model_1():\n",
    "    \"\"\"CNN Model 1: Uses Adam optimizer, deeper layers, and dropout.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # Prevent overfitting\n",
    "        Dense(2, activation='softmax')  # 2 output classes\n",
    "    ])\n",
    "\n",
    "    # Compile model using Adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train CNN Model 1\n",
    "cnn_model_1 = create_cnn_model_1()\n",
    "print(\"\\nTraining CNN Model 1 (Adam, deeper layers, higher dropout)...\")\n",
    "history_1 = cnn_model_1.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "test_loss_1, test_acc_1 = cnn_model_1.evaluate(X_test, y_test)\n",
    "print(f\" CNN Model 1 Test Accuracy: {test_acc_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 3s 124ms/step\n",
      "Confusion Matrix for CNN Model 1:\n",
      "[[350  16]\n",
      " [ 39 414]]\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred_cnn_1 = cnn_model_1.predict(X_test)\n",
    "y_pred_cnn_1_classes = np.argmax(y_pred_cnn_1, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix_cnn_1 = confusion_matrix(y_test_classes, y_pred_cnn_1_classes)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix for CNN Model 1:\")\n",
    "print(conf_matrix_cnn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training CNN Model 2 (SGD, batch norm, fewer layers)...\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 105s 1s/step - loss: 0.7713 - accuracy: 0.7314 - val_loss: 0.9722 - val_accuracy: 0.5739\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 116s 1s/step - loss: 0.4182 - accuracy: 0.8071 - val_loss: 0.7833 - val_accuracy: 0.7106\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 103s 1s/step - loss: 0.3260 - accuracy: 0.8544 - val_loss: 2.4855 - val_accuracy: 0.6349\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 100s 970ms/step - loss: 0.2848 - accuracy: 0.8788 - val_loss: 0.9367 - val_accuracy: 0.7582\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 87s 839ms/step - loss: 0.2319 - accuracy: 0.9038 - val_loss: 0.5936 - val_accuracy: 0.7155\n",
      "26/26 [==============================] - 9s 333ms/step - loss: 0.5936 - accuracy: 0.7155\n",
      " CNN Model 2 Test Accuracy: 0.7155\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- CNN Model 2 --------------------------\n",
    "def create_cnn_model_2():\n",
    "    \"\"\"CNN Model 2: Uses SGD optimizer, batch normalization, and fewer layers.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (5, 5), activation='relu', input_shape=(128, 128, 1)),\n",
    "        BatchNormalization(),  # Normalize activations\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(64, (5, 5), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),  # Lower dropout than model 1\n",
    "        Dense(2, activation='softmax')  # 2 output classes\n",
    "    ])\n",
    "\n",
    "    # Compile model using SGD optimizer (lower learning rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=SGD(learning_rate=0.005, momentum=0.9),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Train CNN Model 2\n",
    "cnn_model_2 = create_cnn_model_2()\n",
    "print(\"\\nTraining CNN Model 2 (SGD, batch norm, fewer layers)...\")\n",
    "history_2 = cnn_model_2.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "test_loss_2, test_acc_2 = cnn_model_2.evaluate(X_test, y_test)\n",
    "print(f\" CNN Model 2 Test Accuracy: {test_acc_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 5s 182ms/step\n",
      "Confusion Matrix for CNN Model 2:\n",
      "[[346  20]\n",
      " [213 240]]\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test set using CNN Model 2\n",
    "y_pred_cnn_2 = cnn_model_2.predict(X_test)\n",
    "y_pred_cnn_2_classes = np.argmax(y_pred_cnn_2, axis=1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix_cnn_2 = confusion_matrix(y_test_classes, y_pred_cnn_2_classes)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix for CNN Model 2:\")\n",
    "print(conf_matrix_cnn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of CNN Models:\n",
      "Model 1 SVC: 0.9304\n",
      "Model 2 MLP neural network: 0.8950\n",
      "Model 1 (Adam, deeper layers) Accuracy: 0.9328\n",
      "Model 2 (SGD, batch norm) Accuracy: 0.7155\n"
     ]
    }
   ],
   "source": [
    "# Compare model performance\n",
    "print(\"\\nComparison of CNN Models:\")\n",
    "print(f\"Model 1 SVC: {svm_accuracy:.4f}\")\n",
    "print(f\"Model 2 MLP neural network: {nn_accuracy:.4f}\")\n",
    "print(f\"Model 1 (Adam, deeper layers) Accuracy: {test_acc_1:.4f}\")\n",
    "print(f\"Model 2 (SGD, batch norm) Accuracy: {test_acc_2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
